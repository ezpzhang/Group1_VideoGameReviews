---
title: "Animal Crossing Text Mining"
author: "John Blank, Scott Yang, and Peter Zhang"
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(RColorBrewer)
library(wordcloud)
if(!require("tensorflow")) install.packages("tensorflow")
library(tensorflow)
if(!require("keras")) install.packages("keras")
```

## EDA

```{r, results = 'hide'}
getwd()

critic.reviews <- data.table::fread("data/critic.csv", stringsAsFactors = F)
user.reviews <- data.table::fread("data/user_reviews.csv", stringsAsFactors = F)

user.reviews$grade <- user.reviews$grade*10 #changing score to be on the same range

par(mfrow=c(1,2))

#Score Distributions of Reviews
boxplot(critic.reviews$grade, ylab="Critic Review out of 100", main="BoxPlot of Critic Scores of Animal Crossing New Horizons")
boxplot(user.reviews$grade, ylab="User Reviews out of 100", main="BoxPlot of User Reviews of Animal Crossing New Horizons")

class(critic.reviews$date)
class(user.reviews$date)

#Review Dates - Release was March 20, 2020
hist(critic.reviews$date, breaks = "days", freq = T, xlab = "Dates")
hist(user.reviews$date, breaks = "days", freq = T, xlab = "Dates")

names(user.reviews)
names(critic.reviews)

#Initial Split. Arbitrarily saying that ratings of 80-100 are good or 1

critic.reviews$judgment <- c(0)
critic.reviews$judgment[critic.reviews$grade >= 85] <- 1 #Games rated higher than 8.5/85 usually considered well reviewed
critic.reviews$judgment <- as.factor(critic.reviews$judgment)
summary(critic.reviews) #str(data)
prop.table(table(critic.reviews$judgment))

user.reviews$judgment <- c(0)
user.reviews$judgment[user.reviews$grade >= 80] <- 1
user.reviews$judgment <- as.factor(user.reviews$judgment)
summary(user.reviews) #str(data)
prop.table(table(user.reviews$judgment))


# Weekdays and Weekends
weekdays_crit <- weekdays(critic.reviews$date) # get weekdays for each review  
months_crit <- months(critic.reviews$date)   # get months 

par(mfrow=c(1,2))
pie(table(weekdays_crit), main="Proportion of Critic reviews") # Pretty much evenly distributed
pie(table(months_crit))  

weekdays_user <- weekdays(user.reviews$date) # get weekdays for each review  
months_user <- months(user.reviews$date)   # get months 

par(mfrow=c(1,2))
pie(table(weekdays_user), main="Proportion of User reviews") # Pretty much evenly distributed
pie(table(months_user))  

#good ratings by weekday and month
prop.table(table(critic.reviews$judgment, weekdays_crit), 2)  # prop of the columns
prop.table(table(critic.reviews$judgment, weekdays_crit), 1)  # critics release reviews on Monday?

prop.table(table(user.reviews$judgment, weekdays_user), 2)  # prop of the columns
prop.table(table(user.reviews$judgment, weekdays_user), 1)  # People write reviews midweek?



```

## Text Analysis

You can also embed plots, for example:

```{r, results = 'hide'}
critic.text <- critic.reviews$text   # take the text out
length(critic.text)
typeof(critic.text)

print(critic.text[1:3]) # view a few documents

user.text <- user.reviews$text   # take the text out
length(user.text)
typeof(user.text)

print(user.text[1:3]) # view a few documents

library(tm)
criticcorpus1 <- VCorpus(VectorSource(critic.text))
criticcorpus1
typeof(criticcorpus1)   ## It is a list
# inspect the first corpus
inspect(criticcorpus1[[10]])
# or use `as.character` to extract the text
as.character(criticcorpus1[[10]])

usercorpus1 <- VCorpus(VectorSource(user.text))
usercorpus1
typeof(usercorpus1)
inspect(usercorpus1[[100]])

lapply(usercorpus1[4:5], as.character)

#tm_map() to clean text

criticcorpus_clean <- tm_map(criticcorpus1, content_transformer(tolower))
usercorpus_clean <- tm_map(usercorpus1, content_transformer(tolower))

criticcorpus_clean <- tm_map(criticcorpus_clean, removeWords, stopwords("english"))
usercorpus_clean <- tm_map(usercorpus_clean, removeWords, stopwords("english"))

#Removing punctuation (skipp for now)
#criticcorpus_clean <- tm_map(criticcorpus_clean, removePunctuation)
#usercorpus_clean <- tm_map(usercorpus_clean, removePunctuation)

# Removes numbers
criticcorpus_clean <- tm_map(criticcorpus_clean, removeNumbers)
usercorpus_clean <- tm_map(usercorpus_clean, removeNumbers)

# Stem Words
criticcorpus_clean <- tm_map(criticcorpus_clean, stemDocument, lazy = TRUE) 
usercorpus_clean <- tm_map(usercorpus_clean, stemDocument, lazy = TRUE)

lapply(usercorpus1[4:5], as.character)

#Document Term Matrix
criticdtm1 <- DocumentTermMatrix(criticcorpus_clean)   ## library = collection of words for all documents
class(criticdtm1)

userdtm1 <- DocumentTermMatrix(usercorpus_clean)   ## library = collection of words for all documents
class(userdtm1)

inspect(criticdtm1)
colnames(userdtm1)[1294:1304]
inspect(criticdtm1[1,]) 
colnames(as.matrix(criticdtm1[1, ]))[which(as.matrix(criticdtm1[1, ]) != 0)]

inspect(userdtm1)
colnames(userdtm1)[16033:16043]
inspect(userdtm1[1,]) 
colnames(as.matrix(userdtm1[1, ]))[which(as.matrix(userdtm1[1, ]) != 0)]


#Long way to make DTM
threshold <- .01*length(criticcorpus_clean)   # 1% of the total documents 
criticwords.10 <- findFreqTerms(criticdtm1, lowfreq=threshold)  # words appearing at least among 1% of the documents
length(criticwords.10)
criticwords.10[413:433]

criticdtm.10<- DocumentTermMatrix(criticcorpus_clean, control = list(dictionary = criticwords.10))  
dim(as.matrix(criticdtm.10))
colnames(criticdtm.10)[423:433]

threshold <- .01*length(usercorpus_clean)   # 1% of the total documents 
userwords.10 <- findFreqTerms(userdtm1, lowfreq=threshold)  # words appearing at least among 1% of the documents
length(userwords.10)
userwords.10[803:823]

userdtm.10<- DocumentTermMatrix(usercorpus_clean, control = list(dictionary = userwords.10))  
dim(as.matrix(userdtm.10))
colnames(userdtm.10)[813:823]

inspect(userdtm.10)

```


```{r, results = 'hide'}
#N-Grams (start with bi-grams)
n <- 2

ngram_tokenizer <- function(x, n) {
  unlist(lapply(ngrams(words(x), 1:n), paste, collapse = "_"), use.names = FALSE)
}

inspect(criticcorpus_clean[[5]])  # see review 5 again
ngram_tokenizer(criticcorpus_clean[[5]], 2)

inspect(usercorpus_clean[[1]])  # see review 5 again
ngram_tokenizer(usercorpus_clean[[1]], 2)

control_list_ngram <- list(tokenize = function(x) ngram_tokenizer(x, 2))


criticdtm_ngram <- DocumentTermMatrix(criticcorpus_clean, control_list_ngram)
userdtm_ngram <- DocumentTermMatrix(usercorpus_clean, control_list_ngram)

# kick out rare words 
criticdtm_ngram.10 <- removeSparseTerms(criticdtm_ngram, 1-.01)  
inspect(criticdtm_ngram.10)

# kick out rare words 
userdtm_ngram.10 <- removeSparseTerms(userdtm_ngram, 1-.01)  
inspect(userdtm_ngram.10)

```

```{r, results='hold'}
#Data Prep
names(critic.reviews)
names(user.reviews)

critic.temp <- data.frame(critic.reviews,as.matrix(criticdtm.10))   
dim(critic.temp)
names(critic.temp)[1:30]

user.temp <- data.frame(user.reviews,as.matrix(userdtm.10))   
dim(user.temp)
names(user.temp)[1:30]

#Data Split
critic.review2 <- critic.temp[,]
names(critic.review2)[1:20]
dim(critic.review2)  ### only 107 entries
user.review2 <- user.temp[,]
names(user.review2)[1:20]
dim(user.review2)  ### only 2999 entries

class(critic.review2$judgment)
class(user.review2$judgment)

table(user.review2$judgment)

#Splitting data into test and train

set.seed(701)  # for the purpose of reproducibility
n_critic <- nrow(critic.review2)
testcritic.index <- sample(n_critic, 25) #only 107 entries so we reserve 25
# length(test.index)
criticreview2.test <- critic.review2[testcritic.index, -c(1:4)] # only keep rating and the texts
criticreview2.train <- critic.review2[-testcritic.index, -c(1:4)]
dim(criticreview2.train) #only text and also removed the 25 samples reserved for testing

n_user <- nrow(user.review2)
testuser.index <- sample(n_user, nrow(user.review2)*0.3) #only 2999 entries so we reserve 30%
# length(test.index)
userreview2.test <- user.review2[testuser.index, -c(1:4)] # only keep rating and the texts
userreview2.train <- user.review2[-testuser.index, -c(1:4)]
dim(userreview2.train) #only text and also removed the 899 samples reserved for testing


```

```{r, results = 'hide'}
#Setting up data for TREES

user.reviewtree <- user.review2[, -c(1:4)]
critic.reviewtree<- critic.review2[,-c(1:4)]
#need to change "..." to plural ellipses or this throws an error
which(colnames(user.reviewtree)=="...")
which(colnames(critic.reviewtree)=="...")

colnames(user.reviewtree)[3] <- "pural ellipses"

#also need to remove special characters like spaces from column names
names(user.reviewtree)<-str_replace_all(names(user.reviewtree), c(" " = "." , "," = "" ))

#also need to remove special characters like spaces from column names
names(critic.reviewtree)<-str_replace_all(names(critic.reviewtree), c(" " = "." , "," = "" ))

```
```{r, results = 'hide'}
# First, we run simple trees for users and critics without splitting the data.
pacman::p_load(tree, rpart, randomForest, ranger, rattle, pROC, partykit, ggplot2, glmnet, lda, data.table, ISLR)

fit.usertree <- tree(judgment~., user.reviewtree, control = tree.control(nrow(user.reviewtree), mindev = 0.005))
plot(fit.usertree)
text(fit.usertree, pretty = TRUE)

fit.usertree2 <- rpart(judgment~., user.reviewtree, minsplit = 1, cp = 9e-3)
plot(as.party(fit.usertree2), main = "Final Tree with Rpart")

```

```{r, results='hide'}
#AUC for the simple user tree with all predictors is 0.8035
predict(fit.usertree2, user.reviewtree)
prob.1 <- predict(fit.usertree2, user.reviewtree)[, 2]
fit1.roc <- roc(user.reviewtree$judgment, prob.1, plot = T)
pROC::auc(fit1.roc)

#AUC for the simple critic review tree is 1.
fit.critictree <- tree(judgment~., critic.reviewtree, control = tree.control(nrow(critic.reviewtree), mindev = 0.005))
plot(fit.critictree)
text(fit.critictree, pretty = TRUE)

fit.critictree2 <- rpart(judgment~., critic.reviewtree, minsplit = 1, cp = 9e-3)
plot(as.party(fit.critictree2), main = "Final Tree with Rpart")

predict(fit.critictree2, critic.reviewtree)
prob.2 <- predict(fit.critictree2, critic.reviewtree)[, 2]
fit2.roc <- roc(critic.reviewtree$judgment, prob.2, plot = T)
pROC::auc(fit2.roc)

```


```{r, results = 'hide'}
# Random Forest with mtry = sqrt(818) ~ 28, ntree = 500
fituser.rf <- randomForest(judgment~., user.reviewtree, mtry = 28, ntree = 500)
plot(fituser.rf)
legend("topright", colnames(fituser.rf$err.rate), col = 1:3, cex = 0.8, fill = 1:3)

```
```{r, results = 'hide'}
fituser.rf.pred <- predict(fituser.rf, user.reviewtree, type="prob")  # output the prob of "0" and "1"
fituser.rf.pred.y <- predict(fituser.rf, user.reviewtree, type="response") # majority vote
fituser.rf.train.err <- mean(user.reviewtree$judgment != fituser.rf.pred.y) #training error =0
fituser.rf.oob.err <- mean(user.reviewtree$judgment != fituser.rf$predicted) 
#Training error for the user random forest is 0.01134, testing misclassification error is 0.148
```

```{r, results = 'hide'}
#Now we run a 2/3 train/test split for the user random forest tree:
set.seed(701)
n <- nrow(user.reviewtree)
n1 <- (2/3)*n
train.index <- sample(n,n1, replace = FALSE)
data.train <- user.reviewtree[train.index, ]
data.test <- user.reviewtree[-train.index, ]

fit.rf.train <- randomForest(judgment~., data.train)
plot(fit.rf.train)
legend("topright", colnames(fit.rf.train$err.rate), col = 1:3, cex=0.8, fill=1:3)

```


```{r, results = 'hide'}
predict.rf.y <- predict(fit.rf.train, newdata=data.test)   # labels
predict.rf <- predict(fit.rf.train, newdata=data.test, type="prob")  #probabilities
# Testing errors
rf.test.err <- mean(data.test$judgment != predict.rf.y)   # didn't set a seed to split the train/test

# Testing ROC curve
roc(data.test$judgment, predict.rf[,2], plot=TRUE)  # Ok
fituser.roc <- roc(data.test$judgment, predict.rf[,2], plot=TRUE)  # Ok
pROC::auc(fituser.roc) #AUC = 0.912 from Randomforest
#AUC for the user random forest is 0.912.

```

```{r, results = 'hide'}
#Now we run the random forest models for the critic reviews with mtry = sqrt (427) ~ 20
fitcritic.rf <- randomForest(judgment~., critic.reviewtree, mtry = 20, ntree = 500) #use mtry = 28 because there are 28^2 p's
plot(fitcritic.rf)
legend("topright", colnames(fitcritic.rf$err.rate), col = 1:3, cex = 0.8, fill = 1:3)

fitcritic.rf.pred <- predict(fitcritic.rf, critic.reviewtree, type="prob")  # output the prob of "0" and "1"
fitcritic.rf.pred.y <- predict(fitcritic.rf, critic.reviewtree, type="response") # majority vote
fitcritic.rf.train.err <- mean(critic.reviewtree$judgment != fitcritic.rf.pred.y) 
fitcritic.rf.oob.err <- mean(critic.reviewtree$judgment != fitcritic.rf$predicted) 
#Training error is 0. Testing Misclassification error is 0.1308

set.seed(701)
#Now we do 2/3 train/test split for the Critic Random Forest:
n <- nrow(critic.reviewtree)
n1 <- (2/3)*n
train.index <- sample(n,n1, replace = FALSE)
data.train <- critic.reviewtree[train.index, ]
data.test <- critic.reviewtree[-train.index, ]

fit.rf.train <- randomForest(judgment~., data.train)
plot(fit.rf.train)
legend("topright", colnames(fit.rf.train$err.rate), col = 1:3, cex=0.8, fill=1:3)

predict.rf.y <- predict(fit.rf.train, newdata=data.test)   # labels
predict.rf <- predict(fit.rf.train, newdata=data.test, type="prob")  #probabilities
# Testing errors
rf.test.err <- mean(data.test$judgment != predict.rf.y)   # didn't set a seed to split the train/test

# Testing ROC curve
roc(data.test$judgment, predict.rf[,2], plot=TRUE)  # Ok
fitcritic.roc <- roc(data.test$judgment, predict.rf[,2])
pROC::auc(fitcritic.roc) #AUC = 0.5707 from Randomforest

#The AUC for the Randomforest generated tree for critic reviews is 0.5707. 
```


```{r, results = 'hide'}
#need to change "..." to plural ellipses or this throws an error
which(colnames(userreview2.train)=="...")
which(colnames(userreview2.test)=="...")

colnames(userreview2.train)[3] <- "pural ellipses"
colnames(userreview2.test)[3] <- "pural ellipses"

#also need to remove special characters like spaces from column names
names(userreview2.train)<-str_replace_all(names(userreview2.train), c(" " = "." , "," = "" ))


#LASSO USER
library(glmnet)

y <- userreview2.train$judgment
X1sparse <- sparse.model.matrix(judgment~., data=userreview2.train)[, -1]
#X1 <- as.matrix(userreview2.train[, -1]) #To be used if there are strange column names
set.seed(701)
resultuser.lasso <- cv.glmnet(X1sparse, y, alpha=.99, family="binomial")
#resultuser.lasso <- resultcritic.lasso
plot(resultuser.lasso)
# this this may take you long time to run, we save result.lasso
saveRDS(resultuser.lasso, file="data/ACuser_lasso.RDS")
# resultuser.lasso can be assigned back by 
# resultuser.lasso <- readRDS("data/ACuser_lasso.RDS")

# non zero words picked up by LASSO
coefuser.1se <- coef(resultuser.lasso, s="lambda.1se")  
lassouser.words <- coefuser.1se@Dimnames[[1]] [coefuser.1se@i][-1] # non-zero variables without intercept. 
summary(lassouser.words)

#RELAXED LASSO

sel_colsuser <- c("judgment", lassouser.words)
data_subuser <- userreview2.train %>% select(all_of(sel_colsuser))
resultuser.glm <- glm(judgment~., family=binomial, data_subuser)

## trim the glm() fat from 
## https://win-vector.com/2014/05/30/trimming-the-fat-from-glm-models-in-r/
stripGlmLR = function(cm) {
  cm$y = c()
  cm$model = c()
  
  cm$residuals = c()
  cm$fitted.values = c()
  cm$effects = c()
  cm$qr$qr = c()  
  cm$linear.predictors = c()
  cm$weights = c()
  cm$prior.weights = c()
  cm$data = c()

  
  cm$family$variance = c()
  cm$family$dev.resids = c()
  cm$family$aic = c()
  cm$family$validmu = c()
  cm$family$simulate = c()
  attr(cm$terms,".Environment") = c()
  attr(cm$formula,".Environment") = c()
  
  cm
}

resultuser.glm.small <- stripGlmLR(resultuser.glm)
saveRDS(resultuser.glm.small, file = "data/ACuser_glm_small.RDS")

#resultuser.lasso.small <- stripGlmLR(resultuser.lasso)

```

```{r, results = 'hide'}
#for critic reviews
#need to change "..." to plural ellipses or this throws an error
which(colnames(criticreview2.train)=="...")
which(colnames(criticreview2.test)=="...")

#colnames(criticreview2.train)[3] <- "pural ellipses"
#colnames(criticreview2.test)[3] <- "pural ellipses"

#also need to remove special characters like spaces from column names
names(criticreview2.train)<-str_replace_all(names(criticreview2.train), c(" " = "." , "," = "" ))

ycritic <- criticreview2.train$judgment
X1criticsparse <- sparse.model.matrix(judgment~., data=criticreview2.train)[, -1]
#X1critic <- as.matrix(criticreview2.train[, -1]) #To be used if there are strange column names
set.seed(701)
resultcritic.lasso <- cv.glmnet(X1criticsparse, ycritic, alpha=.99, family="binomial")
#resultuser.lasso <- resultcritic.lasso
plot(resultcritic.lasso)
# this this may take you long time to run, we save result.lasso
saveRDS(resultcritic.lasso, file="data/ACcritic_lasso.RDS")
# resultcritic.lasso can be assigned back by 
# resultcritic.lasso <- readRDS("data/ACcritic_lasso.RDS")

# non zero words picked up by LASSO critic
coefcritic.1se <- coef(resultcritic.lasso, s="lambda.1se")  
lassocritic.words <- coefcritic.1se@Dimnames[[1]] [coefcritic.1se@i][-1] # non-zero variables without intercept. 
summary(lassocritic.words)

#RELAXED LASSO

sel_colscritic <- c("judgment", lassocritic.words)
data_subcritic <- criticreview2.train %>% select(all_of(sel_colscritic))
resultcritic.glm <- glm(judgment~., family=binomial, data_subcritic)


resultcritic.glm.small <- stripGlmLR(resultcritic.glm)
saveRDS(resultcritic.glm.small, file = "data/ACcritic_glm_small.RDS")

```



```{r, results = 'hide', evaluate = FALSE}
#Word Clouds for Critic reviews (Not viable since all NAs....) We have added the command to not evaluate this block

#resultuser.glm.coef <- coef(resultuser.lasso.small)#Using the full, non-relaxed LASSO
resultcritic.glm.coef <- coef(resultcritic.glm.small)
resultcritic.glm.coef[2:50]
hist(resultcritic.glm.coef)

# pick up the positive coef's which are positively related to the prob of being a good review
goodcritic.glm <- resultcritic.glm.coef[which(resultcritic.glm.coef > 0)]
goodcritic.glm <- goodcritic.glm[-1]  # took intercept out
names(goodcritic.glm)[1:20]  # which words are positively associated with good ratings

goodcritic.fre <- sort(goodcritic.glm, decreasing = TRUE) # sort the coef's
round(goodcritic.fre, 4)[1:20] # leading 20 positive words, amazing!

#hist(as.matrix(goodcritic.fre), breaks=30, col="red") 
goodcritic.word <- names(goodcritic.fre)  # good words with a decreasing order in the coeff's

corcritic.special <- brewer.pal(8,"Dark2")  # set up a pretty color scheme
#wordcloud(goodcritic.word, goodcritic.fre,  # make a word cloud
          #colors=coruser.special, ordered.colors=F)

#Negative Word Cloud

badcritic.glm <- resultcritic.glm.coef[which(resultcritic.glm.coef < 0)]
# names(bad.glm)[1:50]

corcritic.special2 <- brewer.pal(6,"Dark2")
badcritic.fre <- sort(-badcritic.glm, decreasing = TRUE)
round(badcritic.fre, 4)[1:20]

#hist(as.matrix(badcritic.fre), breaks=30, col="green")
badcritic.word <- names(badcritic.fre)
#wordcloud(badcritic.word, badcritic.fre, 
          #color=corcritic.special2, ordered.colors=F)


```

```{r, results = 'hide'}

resultuser.glm.coef <- coef(resultuser.glm.small)
resultuser.glm.coef[2:50]
hist(resultuser.glm.coef)

# pick up the positive coef's which are positively related to the prob of being a good review
gooduser.glm <- resultuser.glm.coef[which(resultuser.glm.coef > 0)]
gooduser.glm <- gooduser.glm[-1]  # took intercept out
names(gooduser.glm)[1:20]  # which words are positively associated with good ratings

gooduser.fre <- sort(gooduser.glm, decreasing = TRUE) # sort the coef's
round(gooduser.fre, 4)[1:20] # leading 20 positive words, amazing!

hist(as.matrix(gooduser.fre), breaks=30, col="red") 
gooduser.word <- names(gooduser.fre)  # good words with a decreasing order in the coeff's


coruser.special <- brewer.pal(8,"Dark2")  # set up a pretty color scheme
wordcloud(gooduser.word, gooduser.fre,  # make a word cloud
          colors=coruser.special, ordered.colors=F)

#Negative Word Cloud

baduser.glm <- resultuser.glm.coef[which(resultuser.glm.coef < 0)]
# names(bad.glm)[1:50]

coruser.special2 <- brewer.pal(6,"Dark2")
baduser.fre <- sort(-baduser.glm, decreasing = TRUE)
round(baduser.fre, 4)[1:20]

hist(as.matrix(baduser.fre), breaks=30, col="green")
baduser.word <- names(baduser.fre)
wordcloud(baduser.word, baduser.fre, 
          color=coruser.special2, ordered.colors=F)



```

```{r, results = 'hide'}
#Predictions for critics
predictcritic.glm <- predict(resultcritic.glm, criticreview2.test, type = "response")
classcritic.glm <- ifelse(predictcritic.glm > .5, "1", "0")
# length(class.glm)

testerrorcritic.glm <- mean(criticreview2.test$judgment != classcritic.glm)
testerrorcritic.glm   # mis classification error is 0.12

pROC::roc(criticreview2.test$judgment, predictcritic.glm, plot=T) # AUC=.0.5 - Not useful


predictcritic.lasso.p <- predict(resultcritic.lasso, as.matrix(criticreview2.test[, -1]), type = "response", s="lambda.1se")
  # output lasso estimates of prob's
predictcritic.lasso <- predict(resultcritic.lasso, as.matrix(criticreview2.test[, -1]), type = "class", s="lambda.1se")
  # output majority vote labels

# LASSO testing errors
mean(criticreview2.test$judgment != predictcritic.lasso)   # .12

# ROC curve for LASSO estimates

pROC::roc(criticreview2.test$judgment, predictcritic.lasso.p, plot=TRUE) #AUC = 0.5 not useful




```



```{r, results = 'hide'}
#Predictions
predictuser.glm <- predict(resultuser.glm, userreview2.test, type = "response")
classuser.glm <- ifelse(predictuser.glm > .5, "1", "0")
# length(class.glm)

testerroruser.glm <- mean(userreview2.test$judgment != classuser.glm)
testerroruser.glm   # mis classification error is 0.21802

pROC::roc(userreview2.test$judgment, predictuser.glm, plot=T) # AUC=.8435!!!!


predictuser.lasso.p <- predict(resultuser.lasso, as.matrix(userreview2.test[, -1]), type = "response", s="lambda.1se")
  # output lasso estimates of prob's
predictuser.lasso <- predict(resultuser.lasso, as.matrix(userreview2.test[, -1]), type = "class", s="lambda.1se")
  # output majority vote labels

# LASSO testing errors
mean(userreview2.test$judgment != predictuser.lasso)   # .1746385

# ROC curve for LASSO estimates

pROC::roc(userreview2.test$judgment, predictuser.lasso.p, plot=TRUE) #AUC = 0.9146


```


```{r, results = 'hide'}
#Neural Net Stuff User use the DTM table...
#Getting a fresh read so we keep 0 and 1s as numerics
user.reviewsint <- data.table::fread("data/user_reviews.csv", stringsAsFactors = F)
user.reviewsint$judgment <- c(0)
user.reviewsint$judgment[user.reviews$grade >= 80] <- 1
class(user.reviewsint$judgment)

#format should be userID, rating, bunch of terms and freq
userfreq <- as.matrix(userdtm.10) #starting with smaller set, words appearing in 1% of documents
dim(userfreq)
class(userfreq[1,3])

class(user.reviews$judgment)
levels(user.reviews$judgment)

which(colnames(userfreq)=="...") #2 has the ...
colnames(userfreq)[2] <- "pural_ellipses"

#also need to remove special characters like spaces from column names
names(userfreq)<-str_replace_all(names(userfreq), c(" " = "." , "," = "" ))

userfreq <- cbind.data.frame(user.reviewsint$judgment,userfreq)#combine user info, ratings, date, etc
colnames(userfreq)[1] <- "judgment"

#userfreq$judgment <- as.numeric(userfreq$judgment)
#userfreq <- as.matrix(userfreq)


##DATA SPLIT
set.seed(701)  # for the purpose of reproducibility
nkuser <- nrow(userfreq)
validationuser.index <- sample(nkuser, 0.3*nkuser)
length(validationuser.index)   # reserve 899
userreview3.val <- userfreq[validationuser.index,] # reserve 899
## validation input/y - this time it is not binomial of good or bad but the actual rating
userreview3_xval <- as.matrix(userreview3.val[, -1])  # make sure it it is a matrix

userreview3_yval <- as.matrix(userreview3.val[, 1]) # make sure it it is a matrix

userreview3_xtrain <- userfreq[-validationuser.index, -1]   #dim(data3_xtrain)
userreview3_ytrain <- as.numeric(userfreq[-validationuser.index, 1])   
userreview3_xtrain <- as.matrix(userreview3_xtrain) # make sure it it is a matrix
userreview3_ytrain <- as.matrix(userreview3_ytrain) # make sure it it is a matrix

#Converting stuff from number to character string. GOTTA FIX

#KERAS Model
library(keras)
puser <- dim(userreview3_xtrain)[2] # number of input variables
modeluser <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(puser)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  # layer 2 with 8 neurons
  #layer_dense(units = 16, activation = "relu") %>% 
  #layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 2, activation = "softmax") # output
print(modeluser)

##COMPILE MODEL USER
modeluser %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

##Set up validation set USER
#set.seed(701)
#valuser_indices <- sample(2100, 210)    # internal testing data size. Training is 2100 entries, 10% reserved
#xuser_val <- userreview3_xtrain[valuser_indices,]    # internal testing data
#partial_xuser_train <- userreview3_xtrain[-valuser_indices,] # training data

#yuser_val <- as.numeric(userreview3_ytrain[valuser_indices])#as.numeric?
#partial_yuser_train <- as.numeric(userreview3_ytrain[-valuser_indices]) #as.numeric?

fituser1 <- modeluser %>% fit(
  userreview3_xtrain,
  userreview3_ytrain,
  epochs = 40,
  batch_size = 512,
  validation_split = 0.15
)

plot(fituser1) #for validation loss, looks like 20 epochs produced best result

#Do you understand NN check
#weights <- modeluser %>% get_weights()

#round(modeluser %>% predict(partial_xuser_train[1:5,]), 3)

#n5 <- 5

# first layer: z_1 = W_1 X + b_1; a_1 = ReLU(z_1)
#z_1 <- partial_xuser_train[1:n5, ] %*% weights[[1]] 
# add beta (weights[[2]]) to every row 
#z_1 <- z_1 + matrix(rep(weights[[2]], n5), nrow = n5, byrow = T)
#a_1 <- matrix(pmax(0, z_1), nrow = n5)

# second layer: z_2 = W_2 a_1 + b_2; a_2 = ReLU(z_2)
#z_2 <- a_1 %*% weights[[3]]
#z_2 <- z_2 + matrix(rep(weights[[4]], n5), nrow = n5, byrow = T)
#a_2 <- matrix(pmax(0,  z_2), nrow = n5)

# output layer: Sigmoid(W_3 a_2 + b_3)
#x.out <- as.numeric(a_2 %*% weights[[5]]) + weights[[6]]
#prob.pred <- 1 / (1 + exp(-x.out)) 

#round(prob.pred, 3)

#Parameter Selection
fituser1$metrics$loss[20]

#prediction (since we stuck with Epoch 20, don't really need to rerun)
rm(partial_y_train, partial_x_train) #Clean up our workspace
puser <- dim(userreview3_xtrain)[2] # number of input variables

#retain the nn:
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(puser)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  # layer 2 with 8 neurons
  #layer_dense(units = 128, activation = "relu") %>%
  layer_dense(units = 2, activation = "softmax") # output

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

model %>% fit(userreview3_xtrain, userreview3_ytrain, epochs = 18, batch_size = 512)

resultsuser <- model %>% evaluate(userreview3_xval, userreview3_yval) ; resultsuser

#prediction user

model %>% predict(userreview3_xval[1:5,])

yuser_pred <- ifelse(model %>% predict(userreview3_xval[1:5,]) > .5, 1, 0)

data.frame(yhat=yuser_pred, y=userreview3_yval[1:5, 1])
#checking to make sure there are no characters
#summary(partial_yuser_train)
#summary(userreview3_xval)
```



```{r, results = 'hide'}
#Neural Net Stuff Critic
#Getting a fresh read so we keep 0 and 1s as numerics
critic.reviewsint <- data.table::fread("data/critic.csv", stringsAsFactors = F)
critic.reviewsint$judgment <- c(0)
critic.reviewsint$judgment[critic.reviews$grade >= 85] <- 1
class(critic.reviewsint$judgment)

#format should be userID, rating, bunch of terms and freq
criticfreq <- as.matrix(criticdtm.10) #starting with smaller set, words appearing in 1% of documents
dim(criticfreq)
class(criticfreq[1,3])

which(colnames(criticfreq)=="...") #no rows have ...
#colnames(userfreq)[2] <- "pural_ellipses"

#also need to remove special characters like spaces from column names
names(criticfreq)<-str_replace_all(names(userfreq), c(" " = "." , "," = "" ))

criticfreq <- cbind.data.frame(critic.reviewsint$judgment,criticfreq)#combine user info, ratings, date, etc
colnames(criticfreq)[1] <- "judgment"


##DATA SPLIT
set.seed(701)  # for the purpose of reproducibility
nkcritic <- nrow(criticfreq)
validationcritic.index <- sample(nkcritic, 0.3*nkcritic)
length(validationcritic.index)   # reserve 32
criticreview3.val <- criticfreq[validationcritic.index,] # 32
## validation input/y - this time it is not binomial of good or bad but the actual rating
criticreview3_xval <- as.matrix(criticreview3.val[, -1])  # make sure it it is a matrix

criticreview3_yval <- as.matrix(criticreview3.val[, 1]) # make sure it it is a matrix

criticreview3_xtrain <- criticfreq[-validationcritic.index, -1]   #dim(data3_xtrain)
criticreview3_ytrain <- as.numeric(criticfreq[-validationcritic.index, 1])   
criticreview3_xtrain <- as.matrix(criticreview3_xtrain) # make sure it it is a matrix
criticreview3_ytrain <- as.matrix(criticreview3_ytrain) # make sure it it is a matrix


#KERAS Model
library(keras)
pcritic <- dim(criticreview3_xtrain)[2] # number of input variables
modelcritic <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(pcritic)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output
print(modelcritic)

##COMPILE MODEL USER
modelcritic %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

#New way of internal validation

fitcritic1 <- modelcritic %>% fit(
  criticreview3_xtrain,
  criticreview3_ytrain,
  epochs = 40,
  batch_size = 512,
  validation_split = 0.15
)

plot(fituser1) #for validation loss, looks like 26 epochs produced good result with lowest validation loss

#Do you understand NN check
#weightscritic <- modelcritic %>% get_weights()

#round(modelcritic %>% predict(criticreview3_xtrain[1:5,]), 3)

#cn5 <- 5

# first layer: z_1 = W_1 X + b_1; a_1 = ReLU(z_1)
#cz_1 <- criticreview3_xtrain[1:cn5, ] %*% weightscritic[[1]] 
# add beta (weights[[2]]) to every row 
#cz_1 <- cz_1 + matrix(rep(weightscritic[[2]], cn5), nrow = cn5, byrow = T)
#ca_1 <- matrix(pmax(0, cz_1), nrow = cn5)

# second layer: z_2 = W_2 a_1 + b_2; a_2 = ReLU(z_2)
#cz_2 <- ca_1 %*% weightscritic[[3]]
#cz_2 <- cz_2 + matrix(rep(weightscritic[[4]], cn5), nrow = cn5, byrow = T)
#ca_2 <- matrix(pmax(0,  cz_2), nrow = cn5)

# output layer: Sigmoid(W_3 a_2 + b_3)
#cx.out <- as.numeric(ca_2 %*% weightscritic[[5]]) + weightscritic[[6]]
#cprob.pred <- 1 / (1 + exp(-cx.out)) 

#round(cprob.pred, 3)

#Parameter Selection
fitcritic1$metrics$loss[10]

#prediction (Using Epoch 7)
#pcritic <- dim(criticreview3_xtrain)[2] # number of input variables

#retain the nn:
modelcritic <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(pcritic)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output

modelcritic %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

modelcritic %>% fit(criticreview3_xtrain, criticreview3_ytrain, epochs = 26, batch_size = 512)

resultscritic <- modelcritic %>% evaluate(criticreview3_xval, criticreview3_yval) ; resultscritic
#loss of 0.2804 and accuracy of 0.9062
#prediction user

modelcritic %>% predict(criticreview3_xval[1:5,])

ycritic_pred <- ifelse(modelcritic %>% predict(criticreview3_xval[1:5,]) > .5, 1, 0)

data.frame(yhat=ycritic_pred, y=criticreview3_yval[1:5, 1])
#checking to make sure there are no characters
#summary(partial_yuser_train)
#summary(userreview3_xval)


```

```{r, results = 'hide', evaluate = FALSE}
#Goofy What-If stuff

#using the user model to run critic data (can't feed different datasets to model probably due to dimensional differences)
library(tokenizers)

user.test <- tokenize_ngrams(user.reviews$text, n=5, n_min = 2, stopwords = stopwords::stopwords("en"))



```